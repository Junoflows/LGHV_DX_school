{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# 한글\n",
    "if platform.system() == \"Windows\":\n",
    "    font_name = font_manager.FontProperties(fname = 'c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "    rc('font', family = font_name)\n",
    "elif platform.system() == \"Darwin\":\n",
    "    rc('font', family = 'AppleGothic')\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   model year  origin                       name  \n",
      "0          70       1  chevrolet chevelle malibu  \n",
      "1          70       1          buick skylark 320  \n",
      "2          70       1         plymouth satellite  \n",
      "3          70       1              amc rebel sst  \n",
      "4          70       1                ford torino  \n"
     ]
    }
   ],
   "source": [
    "# auto_mpg 데이터를 가져와서 horsepower 의 이상치를 제거하고 숫자 타입으로 변환\n",
    "auto_mpg = pd.read_csv('./data4/auto-mpg.csv', header = None)\n",
    "auto_mpg.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin','name']\n",
    "\n",
    "auto_mpg['horsepower'].replace('?', np.nan, inplace = True)\n",
    "auto_mpg.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "auto_mpg['horsepower'] = auto_mpg['horsepower'].astype('float')\n",
    "print(auto_mpg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     저출력  보통출력  고출력\n",
      "0      0     1    0\n",
      "1      0     1    0\n",
      "2      0     1    0\n",
      "3      0     1    0\n",
      "4      0     1    0\n",
      "..   ...   ...  ...\n",
      "393    1     0    0\n",
      "394    1     0    0\n",
      "395    1     0    0\n",
      "396    1     0    0\n",
      "397    1     0    0\n",
      "\n",
      "[392 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 원 핫 인코딩\n",
    "# horsepower 특성을 범주형으로 추가 - 3개의 영역으로 구분\n",
    "count, bin_dividers = np.histogram(auto_mpg['horsepower'], bins = 3)\n",
    "\n",
    "# 각 그룹에 할당할 값의 리스트\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "auto_mpg['hp_bin'] = pd.cut(x = auto_mpg['horsepower'], bins = bin_dividers, labels = bin_names, include_lowest =True)\n",
    "# print(auto_mpg[['horsepower','hp_bin']].head(20))\n",
    "\n",
    "# 원 핫 인코딩 수행 - 값이 3종류 이므로 3개의 특성이 만들어지고 값은 하나만 1이 됨\n",
    "horsepower_dummies = pd.get_dummies(auto_mpg['hp_bin'])\n",
    "print(horsepower_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "['고출력' '보통출력' '저출력']\n"
     ]
    }
   ],
   "source": [
    "# 하나의 특성을 원 핫 인코딩해주는 클래스\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "print(one_hot.fit_transform(auto_mpg['hp_bin']))\n",
    "\n",
    "# 이름을 확인\n",
    "print(one_hot.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 1]\n",
      " [0 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 1]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 1 0]]\n",
      "['C#' 'C++' 'Go' 'Kotlin' 'Python' 'R' 'java']\n"
     ]
    }
   ],
   "source": [
    "# 2개 이상의 특성을 가지고 원 핫 인코딩\n",
    "# 2개 이상의 1이 등장할 수 있음\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multi_features = [('java','C++'), ('C++','Python'), ('java','C#'),\n",
    "                  ('java','Kotlin'), ('Python','Go'), ('Python','R')]\n",
    "\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "\n",
    "print(one_hot_multiclass.fit_transform(multi_features))\n",
    "print(one_hot_multiclass.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  encoder\n",
      "0    저조        1\n",
      "1    보통        2\n",
      "2    보통        2\n",
      "3    저조        1\n",
      "4    우수        3\n",
      "5  매우우수        4\n"
     ]
    }
   ],
   "source": [
    "# 순서가 의미를 갖는 경우 - replace 함수 이용\n",
    "df = pd.DataFrame({'Score' : ['저조', '보통','보통','저조','우수','매우우수']})\n",
    "scale_mapper = {'저조' : 1, '보통' : 2, '우수' : 3, '매우우수' : 4}\n",
    "df['encoder'] = df['Score'].replace(scale_mapper)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [2. 2.]\n",
      " [0. 1.]]\n",
      "[array(['high', 'low', 'normal'], dtype='<U11'), array(['10', '15', '20'], dtype='<U11')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "features = np.array([['low', 10], ['normal', 20], ['high',15]])\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "print(ordinal_encoder.fit_transform(features))\n",
    "print(ordinal_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[[ 0.    0.87  0.31]\n",
      " [ 1.   -0.67 -0.22]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  0.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 모델을 이용한 결측값 대체\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 훈련 데이터 생성\n",
    "X = np.array([[0,2.10,1.45], [1,1.18,1.33],[0,1.22,1.27],[1,-0.21, -1.19]])\n",
    "\n",
    "# 예측에 사용할 데이터\n",
    "X_with_nan = np.array([[np.nan, 0.87, 0.31], [np.nan, -0.67, -0.22]])\n",
    "\n",
    "# KNN 학습기 생성\n",
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "#첫번째 데이터를 label 로 하고 나머지 데이터를 feature 로 설정해서 훈련\n",
    "trained_model = clf.fit(X[:, 1:], X[:, 0])\n",
    "\n",
    "# 예측\n",
    "impute_values = trained_model.predict(X_with_nan[:, 1:])\n",
    "print(impute_values)\n",
    "\n",
    "# 예측한 데이터와 원본 데이터를 합치기\n",
    "X_with_imputed = np.hstack((impute_values.reshape(-1,1), X_with_nan[:, 1:]))\n",
    "print(X_with_imputed)\n",
    "\n",
    "# 결측치를 대체한 데이터와 훈련에 사용한 데이터 합치기\n",
    "result = np.vstack((X_with_imputed, X))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 나오는 데이터로 대체\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_complete = np.vstack((X_with_nan, X))\n",
    "print(X_complete)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='1'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match=' 1'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 매칭 여부를 확인\n",
    "match = re.match('[0-9]', '1234')\n",
    "\n",
    "# 패턴에 일치하는 데이터가 있으면 Match 객체를 리턴하고 없으면 None 을 리턴\n",
    "print(match)\n",
    "\n",
    "match = re.match('[0-9]', 'abc')\n",
    "print(match)\n",
    "\n",
    "match = re.match('\\s[0-9]', ' 1234')\n",
    "# 패턴에 일치하는 데이터가 있으면 Match 객체를 리턴하고 없으면 None 리턴\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@안녕하세요 반갑습니다^^  의미없는 숫자.... !!!!\n",
      " 안녕하세요 반갑습니다 의미없는 숫자 \n"
     ]
    }
   ],
   "source": [
    "string = '@안녕하세요 반갑습니다^^ 123 의미없는 숫자.... !!!!'\n",
    "\n",
    "# 숫자 데이터 제거\n",
    "p = re.compile('[0-9]+')\n",
    "result = p.sub('', string)\n",
    "print(result)\n",
    "\n",
    "# 특수문제 제거\n",
    "p = re.compile('\\W+')\n",
    "result = p.sub(' ', result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 반갑습니다', 'My job is Programmer', 'CC++ C Python']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data = ['안녕하세요 반갑습니다.', 'My job is Programmer', 'C&C++, C#, Python']\n",
    "\n",
    "# 구두점 딕셔너리 생성\n",
    "punctuation =dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "result = [string.translate(punctuation) for string in text_data]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']\n",
      "['품질은 양보다 중요하다.', '한 번의 홈런이 두 번의 2루타보다 낫다.', '혁신은 현존하는 수천가지 것들에 아니라고 말하는 것이다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = 'The science of today is the technology of tomorrow'\n",
    "\n",
    "print(word_tokenize(string))\n",
    "\n",
    "string = '''품질은 양보다 중요하다. 한 번의 홈런이 두 번의 2루타보다 낫다.\n",
    "혁신은 현존하는 수천가지 것들에 아니라고 말하는 것이다.'''\n",
    "\n",
    "print(sent_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1월', '3월', '4월']\n",
      "['chief', 'president', 'kenedy', 'move']\n",
      "['chief', 'president', 'kenedy']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 - 한글은 불용어 사전이 없어서 불용어 사전을 직접 작성\n",
    "word_korean = ['1월','2월','3월','4월']\n",
    "stopwords = ['2월', '5월']\n",
    "result = [i for i in word_korean if i not in stopwords]\n",
    "print(result)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# 영문은 nltk 에서 기본적인 불용어 사전을 제공\n",
    "word_english = ['chief', 'the', 'and', 'an', 'president','kenedy', 'move']\n",
    "result = [w for w in word_english if w not in stopwords.words('english')]\n",
    "print(result)\n",
    "\n",
    "# sklearn이 nltk 보다 불용어의 개수가 조금 더 많음\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "result = [w for w in word_english if w not in ENGLISH_STOP_WORDS]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'lest', 'once', 'person', 'personalization']\n",
      "all\tpython\thave\tpython\tpoorli\tat\tlest\tonc\tperson\tperson\t"
     ]
    }
   ],
   "source": [
    "# 영문 어간 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = \"All pythoners have pythoned poorly at lest once person personalization\"\n",
    "\n",
    "#단어 토큰화 - 공백을 기준으로 분할해서 list 로 생성\n",
    "words = word_tokenize(string)\n",
    "print(words)\n",
    "\n",
    "ps_stemmer = PorterStemmer()\n",
    "# 어간 추출\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('don', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('Persian', 'JJ'), ('cat', 'NN')]\n",
      "['don', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 - 품사 태깅\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# from nltk import pos_tag\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize('The little yellow don barked at the Persian cat')\n",
    "tags_en = pos_tag(tokens)\n",
    "# 단어와 품사의 튜플의 리스트로 출력\n",
    "# 품사(NNP : 고유명사, NN: 명사, RB : 부사, VBD : 동사, VBG: 동사, 동명사, 현재분사, JJ : 형용사)\n",
    "print(tags_en)\n",
    "print([word for word, tag in tags_en if tag in ['NN','NNP']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태양계는 지금으로부터 약 46억 년 전 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다']\n",
      "['태양계', '지금', '46', '46억년', '억', '년', '전', '거대', '분자', '구름', '일부분', '중력', '붕괴', '형성']\n",
      "[('태양계', 'NNG'), ('는', 'JX'), ('지금', 'NNG'), ('으로', 'JKM'), ('부터', 'JX'), ('약', 'MDN'), ('46', 'NR'), ('억', 'NR'), ('년', 'NNB'), ('전', 'NNG'), ('거대', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('분자', 'NNG'), ('구름', 'NNG'), ('의', 'JKG'), ('일부분', 'NNG'), ('이', 'JKS'), ('중력', 'NNG'), ('붕괴', 'NNG'), ('를', 'JKO'), ('일으키', 'VV'), ('면서', 'ECE'), ('형성', 'NNG'), ('되', 'XSV'), ('었', 'EPT'), ('다', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "text = '''태양계는 지금으로부터 약 46억년 전 거대한 분자 구름의 일부분이 중력 붕괴를\n",
    "일으키면서 형성되었다'''\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "# 문장 분석\n",
    "print(kkma.sentences(text))\n",
    "# 단어 분석\n",
    "print(kkma.nouns(text))\n",
    "# 형태소 분석\n",
    "print(kkma.pos(text))\n",
    "\n",
    "# 다른 형태소 분석기\n",
    "# 성능은 Kkma가 우수하다고 알려져 있는데 메모리 사용량이 많고 속도가 조금 느림\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "# 단어 분석\n",
    "print(hannanum.nouns(text))\n",
    "# 형태소 분석\n",
    "print(hannanum.morphs(text))\n",
    "print(hannanum.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best' 'good' 'hate' 'is' 'japan' 'korea' 'live' 'love' 'newziland'\n",
      " 'place' 'sweden' 'to']\n",
      "[[0 0 0 0 0 0 0 1 2 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# BoW(Bag of Word - 단어 개수)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array([\n",
    "    'I love Newziland newziland',\n",
    "    'Sweden is good',\n",
    "    'I hate Japan',\n",
    "    'Korea is best place to live'\n",
    "])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "# print(bag_of_words)\n",
    "# print(bag_of_words.toarray())\n",
    "\n",
    "# 특성 이름 확인\n",
    "print(count.get_feature_names_out())\n",
    "# 밀집 행렬의 형태로 출력됨\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.70710678 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.70710678 0.         0.        ]\n",
      " [0.         0.         0.659118   0.         0.53177225 0.\n",
      "  0.         0.         0.         0.53177225]\n",
      " [0.61418897 0.61418897 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.49552379]\n",
      " [0.         0.         0.         0.52335825 0.42224214 0.52335825\n",
      "  0.         0.         0.52335825 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array([\n",
    "    'I love korea',\n",
    "    'I korea love',\n",
    "    'USA is good',\n",
    "    'USA beats both',\n",
    "    'japan is so hot'\n",
    "])\n",
    "\n",
    "# tf-idf 객체를 생성\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "# feature_matrix\n",
    "\n",
    "print(feature_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
